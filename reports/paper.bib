
@article{goodallPredictingMaterialsProperties2019,
  title = {Predicting Materials Properties without Crystal Structure: {{Deep}} Representation Learning from Stoichiometry},
  author = {Goodall, Rhys E. A. and Lee, Alpha A.},
  year = {2019},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1910.00617},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computational Physics (physics.comp-ph),FOS: Computer and information sciences,FOS: Physical sciences,Machine Learning (cs.LG),Materials Science (cond-mat.mtrl-sci)}
}

@article{goodallPredictingMaterialsProperties2020,
  title = {Predicting Materials Properties without Crystal Structure: {{Deep}} Representation Learning from Stoichiometry},
  shorttitle = {Predicting Materials Properties without Crystal Structure},
  author = {Goodall, Rhys E. A. and Lee, Alpha A.},
  year = {2020},
  month = dec,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  eprint = {1910.00617},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:physics},
  pages = {6280},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-19964-7},
  abstract = {Machine learning has the potential to accelerate materials discovery by accurately predicting materials properties at a low computational cost. However, the model inputs remain a key stumbling block. Current methods typically use descriptors constructed from knowledge of either the full crystal structure -- therefore only applicable to materials with already characterised structures -- or structure-agnostic fixed-length representations hand-engineered from the stoichiometry. We develop a machine learning approach that takes only the stoichiometry as input and automatically learns appropriate and systematically improvable descriptors from data. Our key insight is to treat the stoichiometric formula as a dense weighted graph between elements. Compared to the state of the art for structure-agnostic methods, our approach achieves lower errors with less data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Materials Science,Physics - Computational Physics},
  file = {C\:\\Users\\sterg\\Zotero\\storage\\IF6WR9YR\\Goodall and Lee - 2020 - Predicting materials properties without crystal st.pdf}
}

@misc{kipfSemisupervisedClassificationGraph2016,
  title = {Semi-Supervised Classification with Graph Convolutional Networks},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2016},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1609.02907},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@misc{Pythonista2021,
  title = {Pythonista},
  year = {2021},
  month = nov,
  journal = {Wiktionary},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 64674604},
  file = {C\:\\Users\\sterg\\Zotero\\storage\\8AMZF6UQ\\Pythonista.html}
}

@misc{sahariaPaletteImagetoImageDiffusion2022,
  title = {Palette: {{Image-to-Image Diffusion Models}}},
  shorttitle = {Palette},
  author = {Saharia, Chitwan and Chan, William and Chang, Huiwen and Lee, Chris A. and Ho, Jonathan and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
  year = {2022},
  month = may,
  number = {arXiv:2111.05826},
  eprint = {2111.05826},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io for an overview of the results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\sterg\\Zotero\\storage\\ICZ3654S\\Saharia et al. - 2022 - Palette Image-to-Image Diffusion Models.pdf}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\sterg\\Zotero\\storage\\2TR7873X\\Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@article{wangCompositionallyRestrictedAttentionbased2021,
  title = {Compositionally Restricted Attention-Based Network for Materials Property Predictions},
  author = {Wang, Anthony Yu-Tung and Kauwe, Steven K. and Murdock, Ryan J. and Sparks, Taylor D.},
  year = {2021},
  month = dec,
  journal = {npj Computational Materials},
  volume = {7},
  number = {1},
  pages = {77},
  issn = {2057-3960},
  doi = {10.1038/s41524-021-00545-1},
  abstract = {Abstract             In this paper, we demonstrate an application of the Transformer self-attention mechanism in the context of materials science. Our network, the Compositionally Restricted Attention-Based network (), explores the area of structure-agnostic materials property predictions when only a chemical formula is provided. Our results show that 's performance matches or exceeds current best-practice methods on nearly all of 28 total benchmark datasets. We also demonstrate how 's architecture lends itself towards model interpretability by showing different visualization approaches that are made possible by its design. We feel confident that  and its attention-based framework will be of keen interest to future materials informatics researchers.},
  langid = {english},
  file = {C\:\\Users\\sterg\\Zotero\\storage\\APW9NK4X\\Wang et al. - 2021 - Compositionally restricted attention-based network.pdf}
}

@article{wangCompositionallyrestrictedAttentionbasedNetwork2020,
  title = {Compositionally-Restricted Attention-Based Network for Materials Property Prediction},
  author = {Wang, Anthony and Kauwe, Steven and Murdock, Ryan and Sparks, Taylor},
  year = {2020},
  month = feb,
  publisher = {{American Chemical Society (ACS)}},
  doi = {10.26434/chemrxiv.11869026.v1}
}

@article{xieCrystalGraphConvolutional2017,
  title = {Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties},
  author = {Xie, Tian and Grossman, Jeffrey C.},
  year = {2017},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1710.10324},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Physical sciences,Materials Science (cond-mat.mtrl-sci)}
}

@article{xieCrystalGraphConvolutional2018,
  title = {Crystal {{Graph Convolutional Neural Networks}} for an {{Accurate}} and {{Interpretable Prediction}} of {{Material Properties}}},
  author = {Xie, Tian and Grossman, Jeffrey C.},
  year = {2018},
  month = apr,
  journal = {Physical Review Letters},
  volume = {120},
  number = {14},
  eprint = {1710.10324},
  eprinttype = {arxiv},
  primaryclass = {cond-mat},
  pages = {145301},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.120.145301},
  abstract = {The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with \$10\^4\$ data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Materials Science},
  file = {C\:\\Users\\sterg\\Zotero\\storage\\SCUJG42Y\\Xie_Grossman_2018_Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable.pdf;C\:\\Users\\sterg\\Zotero\\storage\\SJQ6MXAE\\1710.html}
}
